{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyltr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data only from fold 1\n",
    "path = expanduser('~/Downloads/MSLR-WEB10K/Fold1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path + 'train.txt') as trainfile, \\\n",
    "        open(path + 'vali.txt') as valifile, \\\n",
    "        open(path + 'test.txt') as evalfile:\n",
    "    TX, Ty, Tqids, _ = pyltr.data.letor.read_dataset(trainfile)\n",
    "    VX, Vy, Vqids, _ = pyltr.data.letor.read_dataset(valifile)\n",
    "    EX, Ey, Eqids, _ = pyltr.data.letor.read_dataset(evalfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class LambdaMART in module pyltr.models.lambdamart:\n",
      "\n",
      "class LambdaMART(pyltr.models._models.AdditiveModel)\n",
      " |  LambdaMART(metric=None, learning_rate=0.1, n_estimators=100, query_subsample=1.0, subsample=1.0, min_samples_split=2, min_samples_leaf=1, max_depth=3, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=True)\n",
      " |  \n",
      " |  Tree-based learning to rank model.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |  metric : object\n",
      " |      The metric to be maximized by the model.\n",
      " |  learning_rate : float, optional (default=0.1)\n",
      " |      Shrinks the contribution of each tree by `learning_rate`.\n",
      " |      There is a trade-off between learning_rate and n_estimators.\n",
      " |  n_estimators : int, optional (default=100)\n",
      " |      The number of boosting stages to perform. Gradient boosting\n",
      " |      is fairly robust to over-fitting so a large number usually\n",
      " |      results in better performance.\n",
      " |  max_depth : int, optional (default=3)\n",
      " |      Maximum depth of the individual regression estimators. The maximum\n",
      " |      depth limits the number of nodes in the tree. Tune this parameter\n",
      " |      for best performance; the best value depends on the interaction\n",
      " |      of the input variables.\n",
      " |      Ignored if ``max_leaf_nodes`` is not None.\n",
      " |  min_samples_split : int, optional (default=2)\n",
      " |      The minimum number of samples required to split an internal node.\n",
      " |  min_samples_leaf : int, optional (default=1)\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |  subsample : float, optional (default=1.0)\n",
      " |      The fraction of samples to be used for fitting the individual base\n",
      " |      learners. I have no idea why one would set this to something lower than\n",
      " |      one, and results will probably be strange if this is changed from the\n",
      " |      default.\n",
      " |  query_subsample : float, optional (default=1.0)\n",
      " |      The fraction of queries to be used for fitting the individual base\n",
      " |      learners.\n",
      " |  max_features : int, float, string or None, optional (default=None)\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |        - If int, then consider `max_features` features at each split.\n",
      " |        - If float, then `max_features` is a percentage and\n",
      " |          `int(max_features * n_features)` features are considered at each\n",
      " |          split.\n",
      " |        - If \"auto\", then `max_features=sqrt(n_features)`.\n",
      " |        - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |        - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |        - If None, then `max_features=n_features`.\n",
      " |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  max_leaf_nodes : int or None, optional (default=None)\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |      If not None then ``max_depth`` will be ignored.\n",
      " |  verbose : int, optional (default=0)\n",
      " |      Enable verbose output. If 1 then it prints progress and performance\n",
      " |      once in a while (the more trees the lower the frequency). If greater\n",
      " |      than 1 then it prints progress and performance for every tree.\n",
      " |  warm_start : bool, optional (default=False)\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just erase the\n",
      " |      previous solution.\n",
      " |  random_state : int, RandomState instance or None, optional (default=None)\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  feature_importances_ : array, shape = [n_features]\n",
      " |      The feature importances (the higher, the more important the feature).\n",
      " |  oob_improvement_ : array, shape = [n_estimators]\n",
      " |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      " |      relative to the previous iteration.\n",
      " |      ``oob_improvement_[0]`` is the improvement in\n",
      " |      loss of the first stage over the ``init`` estimator.\n",
      " |  train_score_ : array, shape = [n_estimators]\n",
      " |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      " |      model at iteration ``i`` on the in-bag sample.\n",
      " |      If ``subsample == 1`` this is the deviance on the training data.\n",
      " |  estimators_ : ndarray of DecisionTreeRegressor, shape = [n_estimators, 1]\n",
      " |      The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n",
      " |      classification, otherwise n_classes.\n",
      " |  estimators_fitted_ : int\n",
      " |      The number of sub-estimators actually fitted. This may be different\n",
      " |      from n_estimators in the case of early stoppage, trimming, etc.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LambdaMART\n",
      " |      pyltr.models._models.AdditiveModel\n",
      " |      pyltr.models._models.Model\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, metric=None, learning_rate=0.1, n_estimators=100, query_subsample=1.0, subsample=1.0, min_samples_split=2, min_samples_leaf=1, max_depth=3, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=True)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, qids, monitor=None)\n",
      " |      Fit lambdamart onto a dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      \n",
      " |      X : array_like, shape = [n_samples, n_features]\n",
      " |          Training vectors, where n_samples is the number of samples\n",
      " |          and n_features is the number of features.\n",
      " |      y : array_like, shape = [n_samples]\n",
      " |          Target values (integers in classification, real numbers in\n",
      " |          regression)\n",
      " |          For classification, labels must correspond to classes.\n",
      " |      qids : array_like, shape = [n_samples]\n",
      " |          Query ids for each sample. Samples must be grouped by query such\n",
      " |          that all queries with the same qid appear in one contiguous block.\n",
      " |      monitor : callable, optional\n",
      " |          The monitor is called after each iteration with the current\n",
      " |          iteration, a reference to the estimator and the local variables of\n",
      " |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      " |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      " |          is stopped. The monitor can be used for various things such as\n",
      " |          computing held-out estimates, early stopping, model introspecting,\n",
      " |          and snapshoting.\n",
      " |  \n",
      " |  iter_y_delta(self, i, X)\n",
      " |      Calculates target deltas for one iteration of the model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      i : iteration for which to get deltas\n",
      " |      X : array_like of shape = [n_samples, n_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y_delta : array of shape = [n_samples]\n",
      " |          y_delta[j] = ensemble[:i + 1](X[j]) - ensemble[:i](X[j])\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict score for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like of shape = [n_samples, n_features]\n",
      " |          The input samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array of shape [n_samples]\n",
      " |          The predicted scores.\n",
      " |  \n",
      " |  trim(self, n)\n",
      " |      Trim model to first n iterations.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      n : number of iterations to keep\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Return the feature importances (the higher, the more important the\n",
      " |      feature).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array, shape = [n_features]\n",
      " |          Array of summed variance reductions.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pyltr.models._models.Model:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pyltr.models.LambdaMART)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iter  Train score  OOB Improve    Remaining                           Monitor Output \n",
      "    1       0.1786       0.1745       57.35m      C:      0.1803 B:      0.1803 S:  0\n",
      "    2       0.2183       0.0389       64.42m      C:      0.2216 B:      0.2216 S:  0\n",
      "    3       0.2313       0.0084       62.77m      C:      0.2297 B:      0.2297 S:  0\n",
      "    4       0.2354       0.0017       61.15m      C:      0.2315 B:      0.2315 S:  0\n",
      "    5       0.2266       0.0007       60.06m      C:      0.2318 B:      0.2318 S:  0\n",
      "    6       0.2299       0.0007       59.21m      C:      0.2326 B:      0.2326 S:  0\n",
      "    7       0.2281       0.0019       57.79m      C:      0.2340 B:      0.2340 S:  0\n",
      "    8       0.2349       0.0002       56.83m      C:      0.2344 B:      0.2344 S:  0\n",
      "    9       0.2335       0.0002       55.85m      C:      0.2345 B:      0.2345 S:  0\n",
      "   10       0.2346      -0.0002       55.07m      C:      0.2344 B:      0.2345 S:  1\n",
      "   15       0.2597       0.0002       51.85m      C:      0.2638 B:      0.2638 S:  0\n",
      "   20       0.2657       0.0003       48.53m      C:      0.2647 B:      0.2647 S:  0\n",
      "   25       0.3215       0.0003       45.50m      C:      0.3237 B:      0.3237 S:  0\n",
      "   30       0.3353       0.0007       42.35m      C:      0.3325 B:      0.3325 S:  0\n",
      "   35       0.3360      -0.0001       39.48m      C:      0.3338 B:      0.3338 S:  0\n",
      "   40       0.3321       0.0001       36.52m      C:      0.3364 B:      0.3364 S:  1\n",
      "   45       0.3377       0.0004       33.60m      C:      0.3378 B:      0.3378 S:  0\n",
      "   50       0.3389       0.0001       30.68m      C:      0.3431 B:      0.3431 S:  0\n",
      "   60       0.3596       0.0007       24.60m      C:      0.3583 B:      0.3583 S:  0\n",
      "   70       0.3620       0.0005       18.40m      C:      0.3656 B:      0.3656 S:  0\n",
      "   80       0.3671       0.0008       12.21m      C:      0.3684 B:      0.3684 S:  0\n",
      "   90       0.3729       0.0006        6.09m      C:      0.3757 B:      0.3757 S:  0\n",
      "Early termination at iteration  99\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyltr.models.lambdamart.LambdaMART at 0x1a21370d30>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = pyltr.metrics.NDCG(k=10)\n",
    "\n",
    "# Needed when we want to perform validation (early stopping & trimming)\n",
    "monitor = pyltr.models.monitors.ValidationMonitor(\n",
    "    VX, Vy, Vqids, metric=metric, stop_after=15)\n",
    "\n",
    "model = pyltr.models.LambdaMART(\n",
    "    metric=metric,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.02,\n",
    "    max_features=\"auto\",\n",
    "    query_subsample=0.5,\n",
    "    min_samples_leaf=64,\n",
    "    verbose=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(TX, Ty, Tqids, monitor=monitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random ranking: 0.18912831330223595\n",
      "NDCG@10 of our model: 0.3721021688093439\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "Epred = model.predict(EX)\n",
    "print ('Random ranking:', metric.calc_mean_random(Eqids, Ey))\n",
    "print ('NDCG@10 of our model:', metric.calc_mean(Eqids, Ey, Epred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
